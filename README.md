# Тестовое задание на позицию Java-разработчика
Оглавление

1. [Теория gRPC и Protocol Buffers](#Теория) 
2. [Принципиальная схема программы](#компоненты-системы)
3. [Написание кода программы](#детали-реализации)
  
#### Тестовое задание

Написать распределенное файловое хранилище.

**Доступные операции:**

- записать файл. У файла есть имя и путь.

- прочитать файл по переданному пути.

**Контент оригинального файла и прочитанного из этого хранилища должен совпадать.**

**Компоненты системы:**

- координатор - принимает реквесты от клиентов, хранит информацию о путях

- датанода - хранит данные файлов

- клиент - интерфейс взаимодействия пользователя с файловой системой

**Детали реализации**

- Размер файлов не ограничен, для целей задачи можно использовать файлы любого удобного размера.
- Координатор хранит список датанод в памяти. Датанода на старте добавляется в список в координаторе, на остановке - удаляется из списка.
- Координатор должен уметь работать с произвольным числом датанод. Для целей задания хватит 3 датанод.
- Протокол взаимодействия (клиент-координатор, клиент-датанода) - protobuf (grpc).
- Чтобы записать файл, клиент отправляет запрос на координатор, в запросе передается полный путь (с именем) до файла. Если файл уже существует, выдать клиенту ошибку. Если файл не существует, то в ответе координатор возвращает адрес датаноды, куда будет заливаться файл и id загрузки. Клиент заливает данные на переданную датаноду с указанным id загрузки.
- Чтобы прочитать файл, клиент отправляет запрос на координатор, в запросе передается полный путь (с именем) до файла. Если файла нет, выдать клиенту ошибку. Если датанода оффлайн, выдать клиенту ошибку. Если файл есть, то в ответе координатор возвращает адрес датаноды, где хранится файл и id загрузки. Клиент запрашивает файл с переданной датаноды по его id.
- Для целей тестового задания всё можно хранить в памяти (если так удобно)
- Покрыть код unit тестами

**Дополнительное задание (не обязательно)**
- Покрыть код интеграционными тестами.
- Данные на координаторе и датанодах сохранять на диск.
- По факту окончания загрузки клиент отправляет на координатор finalize-реквест. Только финализированные файлы можно прочитать. В фоне на координаторе крутится процесс, которые очищает долго не финализированные файлы как на координаторе, так и на датанодах.
- Заливку и чтение файлов реализовать по частям, размер части конфигурируется. К примеру, размер одной части примем равным 1Мб, файл размером 10Мб будет залит 10 частями.


# План выполнения задания

1) Изучение gRPC и протокола protobuff.
2) Разработка принципиальной схемы программы.
3) Написание кода программы.

<a name="Теория"></a>
# 1) gRPC и protobuff

### Основы gRPC

gRPC - это технология межпроцессного взаимодействия, позволяющая соединять, вызывать, администрировать и отлаживать распределенные приложения в стиле, почти не отличающемся от вызова процедур. В ее основе лежит **RPC** (remote procedure call) - протокол взаимодействия клиента и сервера, при котором клиент вызывает процедуры на сервере удаленно так, как будто они находятся на локальном компьютере. Сервер принимает параметры процедуры по сети, выполняет процедуру и возвращает результат обратно клиенту. 

![j](https://github.com/SKRaidun/DFSTestCase/blob/main/Pictures/1.png)

При разработке RPC-приложения сначала определяют интерфейс сервисов, содержащий информацию о методах, вызывающихся удаленно, передаваемым параметрам и форматам сообщений. На основе интерфейса (или спецификации сервиса) можно сгенерировать код (по типу каркас сервиса),  который затем можно внедрить в логику самого сервиса. Язык для спецификации интерфейса называется IDL - interface definition language.

gRPC использует **Protocol Buffers (protobuff)** в качестве IDL. Это механизм сериализации структурированных данных, не зависящий от платформы. 
Определение сервиса хранится в .proto файле. На основе этого определения с помощью **protoc** компилятора можно сгенерировать серверный и клиентский код. 

- На серверной стороне для обработки клиентских вызовов используется **gRPC-сервер**. 

1. Реализовать логику сервиса на основе сгенерированного кода, пере- определив его базовый класс.

2. Запустить gRPC-сервер, чтобы принимать запросы от клиентов и воз- вращать ответы сервиса.

- С помощью определения сервиса можно сгенерировать **клиентускую заглушку**. Она предоставляет те же методы, что и сервер, но при этом позволяет вызывать клиентский код; далее методы транслируются в удаленные (сетевые) вызовы функций, направленные к серверной стороне.

Когда gRPC-клиент обращается к gRPC-сервису, его gRPC-библиотека выполняет удаленный вызов процедуры по HTTP/2, используя Protocol Buffers и **маршалинг** (упаковываем параметры и удаленные функции в пакет, отправляемый на сервер). Серверная сторона распаковывает полученный запрос и вызывает соответствующую процедуру с помощью Protocol Buffers. Затем аналогичным образом сервер возвращает ответ клиенту. Для передачи данных gRPC задействует HTTP/2.

### Краткие сведения по protobuff

Protocol buffers - это язык определения данных и бинарный формат сериализации. Рассмотрим пример сообщения:

message ID {
	string value = 1;
}

При создании экземпляра сообщения со значением 15, соответствующее байтовое представление выглядит как идентификатор поля value, за которым идет его закодированное значение. Этот идентификатор еще называют тегом. 
Байтовое представление message ID ниже:
![](https://github.com/SKRaidun/DFSTestCase/blob/main/Pictures/Снимок%20экрана%202025-07-17%20в%2000.47.25.png)
Тег содержит индекс поля и транспортный тип. Индекс — уникальное число, которое присваивается каждому полю в определении сообщения внутри proto-файла. Транспортный тип основан на типе поля и определяет то, какого рода данные могут находиться в этом поле; предоставляемая им информация позволяет узнать длину значения. 
Значение поля можно закодировать разными способами:
- При кодировании **переменной длины** старший бит (most significant bit, MSB) каждого байта (кроме последнего) указывает на то, что за ним идут другие байты. Младшие семь бит каждого байта используются для хранения числа, представленного в дополнительном коде (https://ru.wikipedia.org/wiki/ Дополнительный_код). Кроме того, младшая группа идет в самом начале, по- этому к ней следует прибавить бит, обозначающий продолжение значения.
- **Целые числа** со знаком представляют как положительные, так и отрица- тельные значения. К ним относятся такие типы полей, как sint32 и sint64. Сначала эти значения преобразуются в беззнаковые с помощью кодировки zigzag, а затем к ним применяется метод кодирования переменной длины, рассмотренный выше. Zigzag привязывает целое число со знаком к беззнаковому числу, чередуя положительные и отрицательные значения (зигзагом). Для отрицательных целых чисел рекомендуется применять целочисленные типы со знаком, такие как sint32 и sint64, поскольку обычные типы наподобие int32 или int64 преобразуют отрицательные значения в двоичный вид, используя метод кодирования переменной длины. Данный метод требует больше байтов для представления отрицательных чисел по сравнению с положительными. Как следствие, для эффективности отрицательное значение сначала преобразуется в положительное, а затем уже кодируется. Именно этот способ применяется в таких типах, как sint32.
- Числа **фиксированной длины** - противоположность типов переменной длины. Любому значению, независимо от его размера, выделяется одно и то же количество байтов. Для представления чисел фиксированной длины Protocol Buffers использует два транспортных типа: один для 64-битных значений, таких как fixed64, sfixed64 и double, а другой — для 32-битных, таких как fixed32, sfixed32 и float.
- **Строковый тип** принадлежит к транспортному типу ограниченной длины. Это значит, что за значением, закодированным методом переменной длины, идет заданное количество байтов с данными. Для строковых значений используется кодировка UTF-8.

Перед отправкой другой стороне закодированную информацию необходимо  упаковать. Для этого gRPC применяет обрамление сообщений с префиксом длины.

При использовании этого подхода перед записью каждого сообщения указывается его размер. gRPC поддерживает сообщения длиной до 4 Гбайт.

Помимо размера сообщения, при обрамлении также используется однобайт- ное беззнаковое целое число, которое говорит о том, являются ли данные сжатыми. Это так называемый флаг сжатия. Если он равен 1, то двоичные данные были сжаты методом, указанным в HTTP-заголовке Message- Encoding; значение 0 говорит о том, что байты сообщения не сжимались.

![](https://github.com/SKRaidun/DFSTestCase/blob/main/Pictures/Снимок%20экрана%202025-07-17%20в%2001.10.04.png)

### Краткие сведения по HTTP/2

gRPC использует HTTP/2 в качестве своего транспортного протокола для передачи сообщений по сети. Терминология:

- поток — двунаправленная передача данных в рамках установленного соединения; может передавать любое количество сообщений;

- фрейм — наименьшая единица взаимодействия в HTTP/2. Принадлеж- ность каждого фрейма к тому или иному потоку (и, возможно, другая информация) определяется в его заголовке;

- сообщение — полная последовательность фреймов, которая относится к одному логическому HTTP-сообщению. Благодаря этому клиент и сервер могут мультиплексировать сообщения: разбивать их на от- дельные фреймы, перемешивать и затем собирать на другом конце соединения.

HTTP/2 поддерживает мультиплексирование - передача нескольких сообщений в одном соединении.

Когда клиентское приложение создает такой канал, gRPC внутри устанавливает HTTP/2-соединение с сервером. Один и тот же канал можно использовать для отправки серверу любого количества удаленных вызовов. Они привязываются к потокам HTTP/2. Сообщения передаются в виде HTTP/2-фреймов. Фрейм может вмещать одно сообщение с префиксом длины; если же сообщение слишком большое, то может занять несколько фреймов.

![](https://github.com/SKRaidun/DFSTestCase/blob/main/Pictures/Снимок%20экрана%202025-07-17%20в%2001.14.26.png)

**Запрос** — это сообщение, которое инициирует удаленный вызов. В gRPC запрос всегда отправляется клиентским приложением и содержит три основных компонента: заголовок, сообщение с префиксом длины и флаг, обозначающий конец потока. Удаленный вызов инициируется, когда клиент отправляет заголовки запроса. Затем передаются сами сообщения. В конце отправляется флаг EOS (end of stream — «конец потока»), который оповещает получателя о том, что передача запроса завершена.
![](https://github.com/SKRaidun/DFSTestCase/blob/main/Pictures/Снимок%20экрана%202025-07-17%20в%2001.15.32.png)

**Ответ** — это сообщение, возвращаемое сервером после получения клиентского запроса. Как и в предыдущем случае, большинство ответов состоит из трех основных компонентов: заголовков, сообщений с префиксом длины и заключительных блоков. 
![](https://github.com/SKRaidun/DFSTestCase/blob/main/Pictures/Снимок%20экрана%202025-07-17%20в%2001.16.59.png)

### Методы взаимодействия с gRPC

Рассмотрим, как выполняется удаленный вызов процедуры по сети
![](https://github.com/SKRaidun/DFSTestCase/blob/main/Pictures/Снимок%20экрана%202025-07-17%20в%2001.20.50.png)


Существуют разные методы взаимодействия на основе gRPC.

1) **Унарный RPC**
		Клиент, вызывающий удаленную функцию, отправляет серверу один запрос и получает один ответ, содержащий информацию о состоянии и заключительные метаданные. На практике это выглядит так:
		**service OrderManagement {rpc getOrder(google.protobuf.StringValue) returns (Order); }**
2) **Потоковый RPC на стороне сервера**
		Один клиентский запрос генерирует цепочку из несколь- ких ответов. Эту цепочку называют потоком. После возвращения своего последнего ответа сервер передает клиенту заключительные метаданные с информацией о своем состоянии. На практике возвращаем поток:
		**service OrderManagement {rpc searchOrders(google.protobuf.StringValue) returns (stream Order);}**
3) **Потоковый RPC на стороне клиента**
		Клиент отправляет серверу не одно, а несколько сообщений, а тот возвращает один ответ. Но, чтобы ответить, серверу вовсе не обязательно ждать получения всех клиентских запросов. Ответ можно отправить после извлечения из потока одного, нескольких или всех сообщений.
4) **Двунаправленный потоковый RPC**
		В двунаправленном потоковом режиме запрос клиента и ответ сервера представлены в виде потоков сообщений. Вызов должен быть инициирован на клиентской стороне, но дальнейшее взаимодействие зависит лишь от логики клиентского и серверного кода. Пример:
		service OrderManagement {rpc processOrders(stream google.protobuf.StringValue) returns (stream CombinedShipment);}

**В данном задании для стриминга данных по частям (чанками) будут использоваться потоковые RPC на стороне клиента и сервера.**

<a name="компоненты-системы"></a>
# 2) Принципиальная схема программы

Имеем 3 основных компонента:
1) Координатор - хранит метаданные о файлах и доступных нодах.
2) Клиент - интерфейс взаимодействия пользователя с координатором и датанодами.
3) Datanodes - хранит данные.

Рассмотрим каждую из компонент:

1) **Координатор**. Мозг файловой системы, определяющий - куда записывать и откуда брать данные. Основной функционал:
	1) Обработка запросов клиента на создание файла в датаноде. При запросе клиента на запись файла в хранилище координатор должен проверить, имеется ли файл в хранилище. Если имеется - выдаем ошибку. Если нет - координатор выбирает датаноду (одну, не раскидываем части файла по нескольким датанодам), в которую будем записывать файл. Координатор возвращает **id этой датаноды и id загрузки**. 
	2) Обработка запросов клиента на чтение файла из датаноды. Клиент передает координатору полный путь до файла. Если файла нет или датанода оффлайн - выдаем клиенту ошибку. Если файл есть - координатор возвращает **id датаноды и id загрузки**, по которым найдем файл.
	3) Принципиальные моменты:
		1) **Что из себя представляет координатор**? В любом случае координатор возвращает id датаноды и id загрузки файла, которые хранятся в сформированном пути или будут сформированы.  Значит, нужны быстрые операции поиска и записи в необходимую структуру данных. Для таких целей идеально подходит **HashMap<String path, MetaData metadata>**. **Первый аргумент** - полный путь до файла, который передается координатору и по которому осуществляется поиск. **Второй аргумент** - класс метаданных файла, содержайщий id датаноды и id загрузки. Также можно использовать **ArrayList\<Integer datanodeId>**, содержащий в себе активные датаноды. При желании можно будет добавлять еще активных датанод безболезненно.
		2) **Как координатор выбирает датаноду, в которую писать файл?** Мы не балуемся шардированием, поэтому запись файла будет осуществляться в одну датаноду. Также, для записи будут активны сразу 3 датаноды. В таком случае возможные стратегии записи:
			- Рандом.
			- По количеству файлов в ноде.
			- По общему размеру ноды.
			Для простоты реализации выберем равновероятное распределение файлов в системе - рандом. 
2) **Датанода**. Хранит в себе файлы.
	1) Принципиальные моменты:
		1) **Что из себя представляет DataNode?** Если мы храним файлы в память, а не диск, можем использовать хэш-таблицу, какое-нибудь сбалансированное дерево или динамический массив. Имена файлов уникальные (при одинаковом имени выкидываем ошибку). Можно для скорости также использовать хэш-таблицу. 
		2) **В каком формате хранить файл?** Анализ источников показывает, что чаще всего используют байтовый массив. 
		3) **Как записывать файл?** Реализую потоковую запись файла по частям, так как gRPC это поддерживает и проще записывать большие файлы.
3) **Клиент**. Осуществляет запрос на запись или чтение файла. 

<a name="детали-реализации"></a>
# 3) Написание кода программы

### Реализация .proto-файлов для координатора и датаноды.

1) С координатором все ясно - используем унарный RPC во всех методах, так как он принимает filepath и возвращает nodeID и loadID. Метод writefile - запись файла, readFile - чтение файла. Необходимые коды ошибок брал из https://grpc.github.io/grpc/core/md_doc_statuscodes.html.
2) С датанодой интереснее. Так как файлы могут быть любого размера, логично делить файл на чанки, которые будет передавать в потоке. Сервер будет собирать эти чанки в единый файл. С сервера скачивание будет происходить также. **oneof** используется для того, чтобы в первом сообщении передать loadId и dataNode. Для создания потока используем ключевое слово stream (тут используем потоковые RPC на стороне клиента и сервера).

### Реализация логики работы программы

После компиляции написанных .proto-файлов получаем по два класса для каждого сервиса - CoordinatorServiceGrpc/CoordinatorServiceOuterClass и для датаноды. 

#### CoordinatorService

**CoordinatorServiceGrpc** содержит абстрактный класс CoordinatorServiceGrpcImpl, наследуясь от которого мы можем переопределить и написать логику указанных в .proto методов. 

Параметры методов, указанные в .proto файле, содержатся в классе **CoordinatorServiceOuterClass**. Поэтому их мы и передаем в наши методы в Java коде. 

Оба метода принимают объект **responseObserver**,  используемый для возвращения ответа клиенту и закрытия потока.

CoordinatorServiceOuterClass.WriteResponse.Builder responseBuilder= CoordinatorServiceOuterClass.WriteResponse.newBuilder(); - Делаем builder для ответа от сервера. 

**Метод onNext()** - отправить ответ клиенту. 
**Метод onCompleted()** - Закрыть поток и завершить клиентский вызов. 

#### DatanodeService

**writeFile**
Тут мы принимаем чанки от клиента. Для этого возвращаем StreamObserver<DatanodeServiceOuterClass.Chunks>(), в котором переопределяем методы:
- onNext - помним, что на первом месте стоит метадата файла. Обрабатываем этот случай. Дальше пишем в ByteArrayOutputStream инфу из чанка.
- onError - выкидываем ошибку.
- onCompleted - файл собран. Пишем его в мапу со своим loadId. 
- На возвращаемом клиенту streamObserver вызываем onNext для возврата статуса передачи.

**readFile**

Будем возвращать клиенту DatanodeServiceOuterClass.Chunks. Основные моменты - передавать файл мы также будем чанками по 1 Мб. Каждым onNext в цикле передаем чанк. 

#### ClientController

**writeFileRequest**
1) Объявляю и инициализирую экземпляр канала для конкретного nodeId (как стандартный порт 50051 + nodeId). Привяжем сервис DataNodeService к этому порту, чтобы он принимал входящие сообщения.
2) Создаю заглушку для упаковки сообщений. DatanodeServiceGrpc.newStub(dataNodeChannel) -  нельзя создать блокирующую заглушку на потоковом RPC на стороне клиента. 
3) При обработке больших файлов возникала ошибка - они не записывались и из мапы доставался null. Дабы этого избежать решил дожидаться записи в течении 60 сек. Понятно, что при очень больших файлах это не сработает. Также сделал CountDownLatch - блокируем поток, пока не будет ответа onComleted или ошибки.
4) Отправляем первым чанком метаданные.
5) Затем в цикле отправляем сами данные.

**readFileRequest**

1) Объявляю и инициализирую экземпляр сервера для конкретного nodeId (как стандартный порт 50051 + nodeId). Привяжем сервис DataNodeService к этому порту, чтобы он принимал входящие сообщения.
2) Создаю заглушку для упаковки сообщений. DatanodeServiceGrpc.newBlockingStub(dataNodeChannel) - будем ждать ответа от сервера. 
3) Принимаем chunks от dataNode и пишем их в ByteArrayOutputStream.

#### App

Тут определяю gRPC-сервера.

Создаю экземпляр сервера координатора на порте 8080, на котором он будет принимать входящие сообщения. Затем на сервер добавляется реализация сервиса CoordinatorServiceImpl.

Создаю экземпляры серверов для каждой ноды. 

Создаю экземпляр клиента, на котором вызываю методы для записи и чтения файла. 
